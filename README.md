# ğŸ“„ Retrieval-Augmented Generation (RAG) App

## ğŸš€ Overview

This project is a **Retrieval-Augmented Generation (RAG) system** that enables users to upload documents, convert them into embeddings, store them using a vector database (FAISS), and query them using a powerful Large Language Model (LLM). The system is built using **LangChain**, **FAISS**, **SentenceTransformers**, and **Streamlit** for a user-friendly interface.

---

## ğŸ“ Project Structure

```
my_rag_app/
â”‚â”€â”€ app.py                # Streamlit interface
â”‚â”€â”€ .env                  # Environment variables (API keys, model names)
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ document_loader.py # Loads PDF, TXT, CSV, DOCX, XLSX, JSON files
â”‚   â”œâ”€â”€ embedding.py       # Splits documents and generates embeddings
â”‚   â”œâ”€â”€ vectorstore.py     # Stores and retrieves embeddings using FAISS
â”‚   â””â”€â”€ rag_search.py      # Combines retrieval and LLM to answer questions
â”‚â”€â”€ uploaded_data/         # Auto-created folder for uploaded files
â”‚â”€â”€ faiss_store/           # Stores FAISS index and metadata
```

---

## ğŸ§  Component Descriptions

### 1. `document_loader.py`

**Purpose:** Load and convert documents into a unified structure.

* Supports PDF, TXT, CSV, DOCX, Excel, and JSON
* Returns a list of LangChain Document objects

### 2. `embedding.py`

**Purpose:** Transform document text into AI-understandable embeddings.

* Uses SentenceTransformer (default: `all-MiniLM-L6-v2`)
* Splits large documents into chunks
* Generates vector embeddings

### 3. `vectorstore.py`

**Purpose:** Store embeddings for fast search.

* Uses FAISS for similarity search
* Saves embeddings in `faiss.index`
* Saves text metadata in `metadata.pkl`

### 4. `rag_search.py`

**Purpose:** Perform RAG-style search and answer generation.

* Retrieves top relevant chunks
* Sends context to the LLM (Groq API)
* Returns summarized, human-like answers

### 5. `app.py`

**Purpose:** User-friendly interface built with Streamlit.

* Upload documents
* Build knowledge base
* Ask natural language questions
* View AI-generated answers

---

## ğŸ” Environment Setup

Create a `.env` file with the following:

```
GROQ_API_KEY=your_key_here
LLM_MODEL=gemma2-9b-it
EMBEDDING_MODEL=all-MiniLM-L6-v2
PERSIST_DIR=faiss_store
```

---

## â–¶ï¸ How to Run the App

```bash
pip install -r requirements.txt
streamlit run app.py
```

---

## ğŸ’¡ How It Works

1. User uploads documents
2. Documents are chunked and embedded
3. Embeddings are stored in FAISS
4. User asks a question
5. Relevant document chunks are retrieved
6. LLM generates a summarized answer based on retrieved content

---

## ğŸ“¦ Storage Details

| File           | Description                     |
| -------------- | ------------------------------- |
| `faiss.index`  | Stores numeric embeddings       |
| `metadata.pkl` | Stores text chunks and metadata |

---

## ğŸ›  Technologies Used

* **Streamlit** â€“ Frontend interface
* **LangChain** â€“ Document processing & RAG architecture
* **SentenceTransformers** â€“ Embedding generation
* **FAISS** â€“ Vector search
* **Groq LLM** â€“ Answer generation

---

## ğŸŒŸ Features

âœ… Multi-format document support
âœ… Fast semantic search with FAISS
âœ… Retrieval-Augmented responses
âœ… Clean, minimal UI
âœ… Secure API key handling via `.env`

---

## ğŸ“Œ Future Enhancements

* Support chat history
* Add multi-user session storage
* Deploy on cloud platforms

---

## ğŸ¤ Contributions

Feel free to fork this project and submit pull requests to improve features or documentation.

---

## ğŸ“ Contact

For questions or improvements, contact the developer or open an issue.

---

**â­ If you like this project, donâ€™t forget to star the repo!**

# RAG Application using LangChain and Hugging Face

## ğŸ“Œ Overview

This project is a Retrieval-Augmented Generation (RAG) application that allows users to upload documents, embed them using a pre-configured model, and retrieve accurate responses based on document content **without exposing API keys, model names, or vector store details to the end user**.

---

## ğŸš€ Features

* âœ… User-friendly Streamlit interface
* âœ… Secure embedding pipeline (no API key input required)
* âœ… Local vectorstore for fast retrieval
* âœ… Hugging Face or local embedding models
* âœ… Supports PDF, TXT, DOCX uploads
* âœ… Automatic chunking and indexing of documents

---

## ğŸ“‚ Project Structure

| File                 | Description                                           |
| -------------------- | ----------------------------------------------------- |
| `app.py`             | Main Streamlit application for user interface         |
| `document_loader.py` | Loads and processes user-uploaded documents           |
| `embedding.py`       | Handles text embedding using a predefined model       |
| `vectorstore.py`     | Stores embedded document chunks using FAISS or Chroma |
| `utils.py`           | Helper functions                                      |
| `README.md`          | Documentation for the project                         |

---

## ğŸ”§ How It Works

### 1. **User uploads a document**

The document is read and split into smaller chunks using LangChain's `RecursiveCharacterTextSplitter`.

### 2. **Embeddings are generated**

Chunks are embedded using a model defined in the backend (not visible to the user).

### 3. **Vectorstore is created**

Embeddings are stored locally using FAISS or Chroma for fast similarity search.

### 4. **User enters a query**

The query is embedded and matched against the stored vectors to retrieve relevant chunks.

### 5. **Answer generation**

The system combines retrieved information with an LLM to generate a factual, accurate response.

---

## â–¶ï¸ Running the Application

### **Prerequisites**

Make sure you have Python 3.10+ installed.

### **Install dependencies**

```bash
pip install -r requirements.txt
```

### **Start the app**

```bash
streamlit run app.py
```

---

## ğŸ” No API Key Visible to User

* The API key is stored securely in the `.env` file or system environment variables.
* The user never sees or inputs the API key.

Example (`.env` file):

```env
GROQ_API_KEY=your_key_here
```

---

## ğŸ“Š Where Are Documents Stored?

* After embedding, files are stored in a local directory such as `vectorstore/`.
* Each document is saved as part of a FAISS or Chroma index.

Example structure:

```
/vectorstore
    index.faiss
    index.pkl
```

---

## âœ¨ Future Enhancements

* Add chat history memory
* Deploy to cloud (Streamlit Cloud / Hugging Face Spaces)
* Add authentication

---

## ğŸ“ License

This project is licensed under the MIT License.

---

## ğŸ¤ Contributing

Pull requests are welcome! Feel free to open an issue to discuss changes.

---

## ğŸ™Œ Acknowledgements

Built with â¤ï¸ using LangChain, Hugging Face, and FAISS.
